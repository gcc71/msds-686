{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcc71/msds-686/blob/wk-2/MSDS%20686/Week_2/Boston_Housing_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQy8YMFnkXR"
      },
      "source": [
        "# Boston Housing Example\n",
        "This Example was adapted from Deep Learning with Python Chapter 4 Chollet, F. (2021). Deep Learning with Python (2nd ed.). Greenwich, CT, USA: Manning Publications Co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I_8G7ob6nkXW",
        "outputId": "91682724-13c9-45af-a1b2-c6f85a8c97aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aSfEnAnkXi"
      },
      "source": [
        "### This is an example of a neural net regression analysis problem using the prepackaged Boston Housing dataset from the Keras library.  The Boston Housing dataset has only 506 samples. We will use a deep neural net to predict Boston housing prices from 13 features.  You can read more about the dataset here: https://www.kaggle.com/c/boston-housing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FT3JmBMaE04T"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RctHBZqrnkXk",
        "outputId": "0b18d334-cd83-42bd-e165-d2c0cd898a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Import the boston housing data set\n",
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9NnkCFDMnkXq",
        "outputId": "916cbd60-7fa5-4b58-9d4b-7cc7b92f1e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n"
          ]
        }
      ],
      "source": [
        "# A total of 506 samples split between train and test data.  Each have 13 features used in predicting Boston House prices\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q4sJbebfnkXw",
        "outputId": "f7cb8135-9f4a-4dd9-d9a2-380628001043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4]\n"
          ]
        }
      ],
      "source": [
        "# The targets are Boston housing prices in thousands of dollars\n",
        "print(train_targets[1:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XlHIZBWmnkX3",
        "outputId": "7211413f-66e2-4713-ab3c-86249dae40dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.23247e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 6.14200e+00\n",
            "  9.17000e+01 3.97690e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 0.00000e+00 4.15000e-01 7.61000e+00\n",
            "  1.57000e+01 6.27000e+00 2.00000e+00 3.48000e+02 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]\n",
            " [4.89822e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.31000e-01 4.97000e+00\n",
            "  1.00000e+02 1.33250e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.75520e+02\n",
            "  3.26000e+00]\n",
            " [3.96100e-02 0.00000e+00 5.19000e+00 0.00000e+00 5.15000e-01 6.03700e+00\n",
            "  3.45000e+01 5.98530e+00 5.00000e+00 2.24000e+02 2.02000e+01 3.96900e+02\n",
            "  8.01000e+00]\n",
            " [3.69311e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.37600e+00\n",
            "  8.84000e+01 2.56710e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.91430e+02\n",
            "  1.46500e+01]]\n"
          ]
        }
      ],
      "source": [
        "# The features have very different ranges and values\n",
        "print(train_data[0:5,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A_UEFTXOnkX8"
      },
      "outputs": [],
      "source": [
        "# Since all the features have very different ranges and values it is best to normalize the data.\n",
        "# We will subtract the mean value from each feature and divide by one standard deviation.\n",
        "# This will center the data in each feature column around zero with unit standard deviation.\n",
        "# Note: we *don't* compute a separate mean/std. deviation for the test data -- transformations\n",
        "# you apply to your test data should only leverage information learned from the training data,\n",
        "# otherwise you are inadvertently \"peeking\" at your test data, which can give you a false\n",
        "# impression of how well your model generalizes.\n",
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HtUTZXI4nkYB",
        "outputId": "d5da02e8-26d9-4bcc-d1b1-b618b6b92d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "   0.8252202 ]\n",
            " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
            "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
            "  -1.32920239]\n",
            " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
            "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
            "  -1.30850006]\n",
            " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
            "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
            "  -0.65292624]\n",
            " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
            "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
            "   0.26349695]]\n"
          ]
        }
      ],
      "source": [
        "# Now the features have very similar ranges and values\n",
        "print(train_data[0:5,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oJTwfMgxjpK6"
      },
      "outputs": [],
      "source": [
        "# Here's another way to accomplish the same task, using\n",
        "# an off-the-shelf preprocessor provided by the\n",
        "# scikit-learn library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A0ilNSbFnkYH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import backend\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "44R9kzwFnkYM"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(train_data, train_targets, test_size=0.2, shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3RZkrzDbufLP"
      },
      "outputs": [],
      "source": [
        "# Build the model. One hidden layer with 64 hidden units. Since it is a regression analysis the output shape will be 1 (linear)\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YqurQ05UnkYY"
      },
      "outputs": [],
      "source": [
        "# Compile the model. MSE = mean squared error is a typical loss function for regression.  MAE = mean absolute error, a common regression metric\n",
        "model.compile(optimizer = 'rmsprop', loss  = 'mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "whBAFqW-nkYc"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the data.  I set verbose = 0 so we will not see any output.\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 1000,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-x0mykvnkYg"
      },
      "outputs": [],
      "source": [
        "# Use this bit of code to view the History output.\n",
        "hist = pd.DataFrame(history.history)\n",
        "print(hist.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi1dcBlInkYp"
      },
      "outputs": [],
      "source": [
        "#Plot the loss and MAE vs epochs\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "acc_values = history_dict['mae']\n",
        "val_acc_values = history_dict['val_mae']\n",
        "epochs = range(1, len(history_dict['mae']) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaSQiWkgnkYt"
      },
      "outputs": [],
      "source": [
        "#Plot the loss epochs\n",
        "plt.plot(epochs, loss_values, 'r', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "plt.ylim(0,20)\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ozqqvOnkY0"
      },
      "outputs": [],
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(0,5)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuZzFD0kwqlc"
      },
      "outputs": [],
      "source": [
        "#Plot MAE vs epochs\n",
        "plt.plot(epochs, acc_values, 'r', label = 'Training MAE')\n",
        "plt.plot(epochs, val_acc_values, 'b', label = 'Validation MAE')\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,200)\n",
        "plt.title('Training and validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiaQGbFKnkY7"
      },
      "outputs": [],
      "source": [
        "# Apply the model to the test data and view the results.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtlwzqXXnkY_"
      },
      "outputs": [],
      "source": [
        "# It looks like validation loss and mean absolute error increase after about 200 epochs.\n",
        "# Lets remake the model with only 200 epochs\n",
        "\n",
        "backend.clear_session()\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                   y_train,\n",
        "                   epochs = 200,\n",
        "                   batch_size=16,\n",
        "                   validation_data=(x_valid, y_valid),\n",
        "                   verbose = 0)\n",
        "\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hx6DOYB3Hat"
      },
      "source": [
        "### We will incorporate K-Fold cross validation to help improve our model\n",
        "Since we have so few data points it makes sence to train and validate on all the data in k-fold batches. We will use `scikit-learn` to do the [k-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html ).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwzO697PnkYR"
      },
      "outputs": [],
      "source": [
        "# This time when we build the model we want to wrap it in a function because\n",
        "# we will be building a model for each k-fold of our validation steps.\n",
        "# To make sure we are comparing apples to apples lets use the same parameters\n",
        "# we used above.\n",
        "\n",
        "def build_model():\n",
        "  backend.clear_session()\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer = 'adam', loss  = 'mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZixWlotOh-hr"
      },
      "outputs": [],
      "source": [
        "# Now we program in the KFold validation from sklearns\n",
        "# We will use 4 Kfold steps for validation on the original train data\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k = 4 # Number of splits\n",
        "kf = KFold(n_splits = k, shuffle=True) # Call the KFold function\n",
        "all_val_mae_scores=[] # Where we will save all of the outputs\n",
        "num_epochs = 1000\n",
        "num_batch = 16\n",
        "# We have to run a loop and save all the mae scores from each of our 4\n",
        "# KFold splits\n",
        "for k_train_index, k_val_index in kf.split(train_data, train_targets):\n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train_index], train_targets[k_train_index],\n",
        "                      validation_data = (train_data[k_val_index], train_targets[k_val_index]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)\n",
        "  all_val_mae_scores.append(history.history['val_mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0E2wUn0tFrx"
      },
      "outputs": [],
      "source": [
        "# Look at all the scores\n",
        "# all_val_mae_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPu7EoMbwZvW"
      },
      "outputs": [],
      "source": [
        "# We need to average each of the 4 KFold mae scores\n",
        "average_val_mae = [np.mean([x[i] for x in all_val_mae_scores]) for i in range(num_epochs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrzJurYgDV85"
      },
      "outputs": [],
      "source": [
        "# Plot the average Val\n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim()\n",
        "plt.xlim()\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXaSZu_wy4J"
      },
      "outputs": [],
      "source": [
        "# Lets zoom in on that plot to find the optimal number of epochs to run.\n",
        "plt.plot(range(1, len(average_val_mae) + 1), average_val_mae)\n",
        "plt.ylim(2,3)\n",
        "plt.xlim(50,400)\n",
        "plt.title('Average Validation MAE from K-Fold CV')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Val MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baJRUiq-xcc5"
      },
      "outputs": [],
      "source": [
        "# Rerun the model with the optimal epochs.\n",
        "k = 4\n",
        "kf = KFold(n_splits = k)\n",
        "all_mae_scores=[]\n",
        "num_epochs = 225\n",
        "num_batch = 16\n",
        "for k_train, k_val in kf.split(train_data, train_targets):\n",
        "  model = build_model()\n",
        "  history = model.fit(train_data[k_train], train_targets[k_train],\n",
        "                      validation_data = (train_data[k_val], train_targets[k_val]),\n",
        "                      epochs = num_epochs, batch_size=num_batch, verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlKrAzuk3GNe"
      },
      "outputs": [],
      "source": [
        "# Evaluate our model on the test data.\n",
        "results = model.evaluate(test_data, test_targets)\n",
        "print(results)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWl3UV2q_qVo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Boston_Housing_Example.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}